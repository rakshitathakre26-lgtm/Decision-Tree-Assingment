{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {
        "id": "mHdkS0OaULh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of classification?"
      ],
      "metadata": {
        "id": "rpJcbF-hUSDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is a Decision Tree?\n",
        "\n",
        "A Decision Tree is a machine learning model that makes decisions by asking a series of questions, just like how we make decisions in real life.\n",
        "\n",
        "It looks like an inverted tree:\n",
        "\n",
        "Root node ‚Üí first question\n",
        "\n",
        "Branches ‚Üí possible answers\n",
        "\n",
        "Leaf nodes ‚Üí final decision (class label)\n",
        "\n",
        "‚úÖ How it works in classification\n",
        "\n",
        "In classification, the goal is to predict a category (like ‚Äúspam or not spam‚Äù, ‚Äúdiabetic or not‚Äù, ‚Äúapple or orange‚Äù).\n",
        "\n",
        "A Decision Tree works in these steps:\n",
        "\n",
        "1. Start at the root\n",
        "\n",
        "It begins with the whole dataset and picks the best feature to split the data.\n",
        "Example: For predicting if a fruit is ‚ÄúApple‚Äù or ‚ÄúOrange‚Äù, the first question could be:\n",
        "‚ÄúIs the color orange?‚Äù\n",
        "\n",
        "2. Split into branches\n",
        "\n",
        "Depending on the answer (‚ÄúYes‚Äù or ‚ÄúNo‚Äù), the data is divided into groups.\n",
        "\n",
        "3. Continue asking questions\n",
        "\n",
        "Each group is further split based on another best feature (like weight, size, texture).\n",
        "\n",
        "4. Reach the leaf node\n",
        "\n",
        "When the tree cannot be split further, it decides the final class label.\n",
        "\n",
        "For example:\n",
        "\n",
        "If color = orange ‚Üí Orange\n",
        "\n",
        "If color ‚â† orange and weight < some value ‚Üí Apple\n",
        "\n",
        "‚úÖ How does the tree decide which question to ask?\n",
        "\n",
        "It uses mathematical criteria like:\n",
        "\n",
        "Gini impurity\n",
        "\n",
        "Entropy / Information Gain\n",
        "\n",
        "These help choose the feature that best separates the classes.\n",
        "\n",
        "‚úÖ Simple Example\n",
        "\n",
        "Suppose you want to classify whether a student will ‚ÄúPass‚Äù or ‚ÄúFail‚Äù based on study hours:\n",
        "\n",
        "Root:\n",
        "‚û°Ô∏è Did the student study ‚â• 3 hours?\n",
        "\n",
        "Yes ‚Üí Pass\n",
        "\n",
        "No ‚Üí Fail\n",
        "\n",
        "Just like that, a tree uses simple questions to classify.\n",
        "\n",
        "‚úÖ Summary\n",
        "\n",
        "A Decision Tree:\n",
        "\n",
        "is a flowchart-like model\n",
        "\n",
        "works by asking the best possible questions\n",
        "\n",
        "splits data step-by-step\n",
        "\n",
        "ends with a final classification decision"
      ],
      "metadata": {
        "id": "oxaoNSVRUcZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.How do they impact the splits in a Decision Tree?\n"
      ],
      "metadata": {
        "id": "7H6wWmRuUxo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Question 2: What are Gini Impurity and Entropy? How do they affect Decision Tree splits?\n",
        "\n",
        "Decision Trees try to split the data in such a way that each group becomes as pure as possible.\n",
        "\n",
        "A pure node = contains only one class\n",
        "Example: All ‚ÄúApple‚Äù or all ‚ÄúOrange‚Äù.\n",
        "\n",
        "To measure how impure a node is, we use impurity measures:\n",
        "‚úÖ Gini Impurity\n",
        "‚úÖ Entropy\n",
        "\n",
        "‚úÖ 1. Gini Impurity\n",
        "Definition\n",
        "\n",
        "Gini impurity tells us how mixed the classes are in a node.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Gini\n",
        "=\n",
        "1\n",
        "‚àí\n",
        "‚àë\n",
        "ùëù\n",
        "ùëñ\n",
        "2\n",
        "Gini=1‚àí‚àëp\n",
        "i\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "Where\n",
        "ùëù\n",
        "ùëñ\n",
        "p\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        " = probability of class\n",
        "ùëñ\n",
        "i\n",
        "\n",
        "Intuition\n",
        "\n",
        "0 ‚Üí perfectly pure (only one class)\n",
        "\n",
        "Higher value ‚Üí more mixed\n",
        "\n",
        "Example\n",
        "\n",
        "If a node has:\n",
        "\n",
        "50% apples\n",
        "\n",
        "50% oranges\n",
        "\n",
        "Gini\n",
        "=\n",
        "1\n",
        "‚àí\n",
        "(\n",
        "0.5\n",
        "2\n",
        "+\n",
        "0.5\n",
        "2\n",
        ")\n",
        "=\n",
        "0.5\n",
        "Gini=1‚àí(0.5\n",
        "2\n",
        "+0.5\n",
        "2\n",
        ")=0.5\n",
        "\n",
        "If a node has:\n",
        "\n",
        "100% apples\n",
        "\n",
        "Gini\n",
        "=\n",
        "1\n",
        "‚àí\n",
        "(\n",
        "1\n",
        "2\n",
        ")\n",
        "=\n",
        "0\n",
        "Gini=1‚àí(1\n",
        "2\n",
        ")=0\n",
        "\n",
        "(pure)\n",
        "\n",
        "‚úÖ 2. Entropy\n",
        "Definition\n",
        "\n",
        "Entropy measures the uncertainty or disorder in a node.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Entropy\n",
        "=\n",
        "‚àí\n",
        "‚àë\n",
        "ùëù\n",
        "ùëñ\n",
        "log\n",
        "‚Å°\n",
        "2\n",
        "ùëù\n",
        "ùëñ\n",
        "Entropy=‚àí‚àëp\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "log\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "p\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "Intuition\n",
        "\n",
        "0 ‚Üí pure\n",
        "\n",
        "Higher entropy ‚Üí more disorder (more mixed)\n",
        "\n",
        "Example\n",
        "\n",
        "50% apples, 50% oranges:\n",
        "\n",
        "Entropy\n",
        "=\n",
        "‚àí\n",
        "[\n",
        "0.5\n",
        "log\n",
        "‚Å°\n",
        "2\n",
        "0.5\n",
        "+\n",
        "0.5\n",
        "log\n",
        "‚Å°\n",
        "2\n",
        "0.5\n",
        "]\n",
        "=\n",
        "1\n",
        "Entropy=‚àí[0.5log\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "0.5+0.5log\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "0.5]=1\n",
        "\n",
        "100% apples ‚Üí Entropy = 0 (pure)\n",
        "\n",
        "‚úÖ How do Gini & Entropy impact splits in a Decision Tree?\n",
        "\n",
        "A decision tree tries to split data such that impurity decreases after the split.\n",
        "\n",
        "‚úÖ Goal of every split\n",
        "\n",
        "Choose the feature that produces child nodes with the lowest impurity.\n",
        "\n",
        "‚úÖ Information Gain (based on Entropy)\n",
        "\n",
        "If entropy is used, the tree chooses the split with the highest Information Gain:\n",
        "\n",
        "Information Gain\n",
        "=\n",
        "Entropy(before split)\n",
        "‚àí\n",
        "Entropy(after split)\n",
        "Information Gain=Entropy(before split)‚àíEntropy(after split)\n",
        "\n",
        "More gain ‚Üí better split.\n",
        "\n",
        "‚úÖ How Gini vs Entropy change the splits\n",
        "Measure\tWhat it focuses on\tBehaviour\n",
        "Gini Impurity\tProbability of misclassification\tSplits faster, computationally simpler\n",
        "Entropy\tUncertainty / information gain\tMore accurate but slightly slower\n",
        "Both\tPrefer pure splits\tGive very similar results in practice\n",
        "‚úÖ Example (Simple)\n",
        "\n",
        "Suppose you can split the data using:\n",
        "\n",
        "Color\n",
        "\n",
        "Weight\n",
        "\n",
        "The tree calculates:\n",
        "\n",
        "Gini/entropy before the split\n",
        "\n",
        "Gini/entropy of child nodes after the split\n",
        "\n",
        "Whichever feature results in the largest reduction in impurity becomes the split.\n",
        "\n",
        "‚úÖ Summary\n",
        "\n",
        "Gini Impurity and Entropy measure how mixed a node is.\n",
        "\n",
        "Lower values = purer nodes.\n",
        "\n",
        "Decision Trees choose the feature that gives:\n",
        "\n",
        "Lowest impurity, OR\n",
        "\n",
        "Highest information gain (entropy)\n",
        "\n",
        "Gini is simpler; entropy gives similar but more information-focused splits."
      ],
      "metadata": {
        "id": "BWF-HlmiU5Xu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n"
      ],
      "metadata": {
        "id": "xmgrwrcpVFgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Question 3: Difference between Pre-Pruning and Post-Pruning in Decision Trees\n",
        "\n",
        "Decision Trees can easily become too deep and overfit the training data.\n",
        "To prevent this, we use pruning, which means stopping the tree from becoming unnecessarily large.\n",
        "\n",
        "There are two types:\n",
        "\n",
        "‚úÖ Pre-Pruning (Early Stopping)\n",
        "\n",
        "‚úÖ Post-Pruning (Prune after full growth)\n",
        "\n",
        "‚úÖ 1. Pre-Pruning (Early Stopping)\n",
        "Definition\n",
        "\n",
        "Pre-pruning stops the tree while it is still growing.\n",
        "We apply certain conditions to stop the splitting early.\n",
        "\n",
        "Common pre-pruning methods:\n",
        "\n",
        "Set a maximum depth (e.g., max_depth = 5)\n",
        "\n",
        "Minimum samples required to split (min_samples_split)\n",
        "\n",
        "Minimum samples in a leaf node (min_samples_leaf)\n",
        "\n",
        "Practical Advantage\n",
        "\n",
        "‚úÖ Faster training ‚Üí because the tree doesn‚Äôt grow fully.\n",
        "Useful when working with large datasets or limited computational resources.\n",
        "\n",
        "‚úÖ 2. Post-Pruning (Pruning after full growth)\n",
        "Definition\n",
        "\n",
        "Post-pruning allows the tree to grow completely first, and then removes branches that do not improve accuracy.\n",
        "\n",
        "Common post-pruning techniques:\n",
        "\n",
        "Cost Complexity Pruning (used in scikit-learn, parameter: ccp_alpha)\n",
        "\n",
        "Reduced-error pruning (older method)\n",
        "\n",
        "Practical Advantage\n",
        "\n",
        "‚úÖ Improves generalization ‚Üí because unnecessary branches are removed after evaluating accuracy.\n",
        "Often gives more accurate models than pre-pruning.\n",
        "\n",
        "‚úÖ Main Differences Summary\n",
        "Feature\tPre-Pruning\tPost-Pruning\n",
        "When applied?\tDuring tree growth\tAfter full tree is built\n",
        "How?\tStops splits early\tRemoves weak branches\n",
        "Goal\tPrevent complexity\tSimplify after evaluation\n",
        "Advantage\tFaster training\tBetter accuracy & generalization"
      ],
      "metadata": {
        "id": "_DN8uIoMVLd-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?"
      ],
      "metadata": {
        "id": "8tontPcIVSw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What is Information Gain?\n",
        "\n",
        "Information Gain (IG) is a measure used in Decision Trees to decide which feature to split on at each step.\n",
        "\n",
        "It tells us how much ‚Äúinformation‚Äù a split adds by reducing uncertainty (entropy).\n",
        "\n",
        "Formula:\n",
        "Information Gain\n",
        "=\n",
        "Entropy(before split)\n",
        "‚àí\n",
        "Entropy(after split)\n",
        "Information Gain=Entropy(before split)‚àíEntropy(after split)\n",
        "Intuition\n",
        "\n",
        "High Information Gain ‚Üí the split makes the data purer\n",
        "\n",
        "Low Information Gain ‚Üí the split does not improve purity much\n",
        "\n",
        "‚úÖ Why is Information Gain important?\n",
        "\n",
        "A Decision Tree must choose the best question (feature) at every node.\n",
        "\n",
        "Information Gain helps by:\n",
        "\n",
        "‚úÖ showing which feature reduces impurity the most\n",
        "\n",
        "‚úÖ ensuring the tree asks the most informative question first\n",
        "\n",
        "‚úÖ making the model accurate and efficient\n",
        "\n",
        "Example:\n",
        "\n",
        "If splitting on Color gives 80% purity\n",
        "and splitting on Size gives only 40% purity,\n",
        "‚Üí The tree will choose Color because it has higher Information Gain.\n",
        "\n",
        "‚úÖ In short:\n",
        "\n",
        "Information Gain = how much entropy decreases after a split.\n",
        "\n",
        "The decision tree selects the feature with the highest information gain.\n",
        "\n",
        "This ensures the tree becomes simpler, more accurate, and less confusing."
      ],
      "metadata": {
        "id": "RNYF6s6ZVeiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?"
      ],
      "metadata": {
        "id": "F6w_zkOEVg4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Question 5: Real-World Applications of Decision Trees + Advantages & Limitations\n",
        "‚úÖ Real-World Applications of Decision Trees\n",
        "1. Medical Diagnosis\n",
        "\n",
        "Decision Trees help doctors classify whether a patient has a certain disease based on symptoms, test results, and medical history.\n",
        "\n",
        "2. Banking & Finance\n",
        "\n",
        "Used for:\n",
        "\n",
        "credit scoring (approve or reject loans)\n",
        "\n",
        "identifying risky customers\n",
        "\n",
        "fraud detection\n",
        "\n",
        "3. Marketing & Customer Segmentation\n",
        "\n",
        "Companies use trees to:\n",
        "\n",
        "predict which customers will buy a product\n",
        "\n",
        "target advertisements\n",
        "\n",
        "analyze customer behavior\n",
        "\n",
        "4. E-commerce & Recommendation Systems\n",
        "\n",
        "Used to predict:\n",
        "\n",
        "whether a user will click on a product\n",
        "\n",
        "what items to recommend\n",
        "\n",
        "5. Manufacturing & Quality Control\n",
        "\n",
        "To detect defective products by checking measurements, weight, or color.\n",
        "\n",
        "6. Weather Prediction\n",
        "\n",
        "Classifying whether it will rain or not based on temperature, humidity, pressure, etc.\n",
        "\n",
        "‚úÖ Main Advantages of Decision Trees\n",
        "1. Easy to Understand & Interpret\n",
        "\n",
        "Looks like a flowchart ‚Äî even non-technical people can understand the logic.\n",
        "\n",
        "2. Works with Both Numerical and Categorical Data\n",
        "\n",
        "Handles numbers (e.g., age, salary) and categories (e.g., color, gender) easily.\n",
        "\n",
        "3. Requires Little Data Preprocessing\n",
        "\n",
        "No need for:\n",
        "\n",
        "feature scaling\n",
        "\n",
        "normalization\n",
        "\n",
        "dummy variables (scikit-learn handles categories internally)\n",
        "\n",
        "4. Fast and Efficient\n",
        "\n",
        "Tree construction is quick and works well even for large datasets.\n",
        "\n",
        "‚úÖ Main Limitations of Decision Trees\n",
        "1. Highly Prone to Overfitting\n",
        "\n",
        "If not pruned, the tree becomes too deep and memorizes the training data.\n",
        "\n",
        "2. Unstable\n",
        "\n",
        "A small change in data can produce a completely different tree.\n",
        "\n",
        "3. May Create Biased Trees\n",
        "\n",
        "If one feature has many levels (e.g., zip code), the tree may prefer it even if not meaningful.\n",
        "\n",
        "4. Not the Best for Continuous Predictions\n",
        "\n",
        "Decision Trees for regression often give block-like, non-smooth predictions.\n",
        "\n",
        "‚úÖ In Short\n",
        "‚úÖ Applications:\n",
        "\n",
        "Medical diagnosis, finance, marketing, e-commerce, manufacturing, weather, etc.\n",
        "\n",
        "‚úÖ Advantages:\n",
        "\n",
        "Easy to interpret, fast, handles all data types, minimal preprocessing.\n",
        "\n",
        "‚úÖ Limitations:\n",
        "\n",
        "Overfitting, instability, bias, not smooth for regression."
      ],
      "metadata": {
        "id": "0G0QmtmhVmfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Info:\n",
        "‚óè Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "‚óè Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV)."
      ],
      "metadata": {
        "id": "3v3p0Ro8VuXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "‚óè Train a Decision Tree Classifier using the Gini criterion\n",
        "‚óè Print the model‚Äôs accuracy and feature importances"
      ],
      "metadata": {
        "id": "NwpsJ8NOWK8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Python Program: Decision Tree on Iris Dataset (Gini criterion)"
      ],
      "metadata": {
        "id": "lKtOK5SDWL8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data        # features\n",
        "y = iris.target      # labels\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Decision Tree Classifier using Gini criterion\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5. Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# 6. Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(iris.feature_names, model.feature_importances_):\n",
        "    print(f\"{name}: {importance}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BWDHwBjWU6W",
        "outputId": "e48052fd-efb3-461f-f845-07f9a00e015f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0\n",
            "sepal width (cm): 0.01911001911001911\n",
            "petal length (cm): 0.8932635518001373\n",
            "petal width (cm): 0.08762642908984374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What the Program Does\n",
        "\n",
        "Loads the Iris dataset\n",
        "\n",
        "Splits it into training (70%) and testing (30%)\n",
        "\n",
        "Trains a Decision Tree Classifier using Gini Impurity\n",
        "\n",
        "Prints:\n",
        "\n",
        "‚úÖ Model accuracy\n",
        "\n",
        "‚úÖ Feature importances (which features were most important for the tree)"
      ],
      "metadata": {
        "id": "Nq62guzbWhVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "‚óè Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree."
      ],
      "metadata": {
        "id": "2RIkThJ7WmF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a clean, ready-to-run Python program that trains:\n",
        "\n",
        "‚úÖ a fully-grown Decision Tree\n",
        "‚úÖ a Decision Tree with max_depth = 3\n",
        "\n",
        "‚Ä¶and compares their accuracies.\n",
        "\n",
        "‚úÖ Python Program: Compare Full Tree vs max_depth=3"
      ],
      "metadata": {
        "id": "MpJs5agMWsTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split into training and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Fully-grown Decision Tree (no max_depth limit)\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "full_pred = full_tree.predict(X_test)\n",
        "full_acc = accuracy_score(y_test, full_pred)\n",
        "\n",
        "# 4. Decision Tree with max_depth = 3\n",
        "limited_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "limited_tree.fit(X_train, y_train)\n",
        "limited_pred = limited_tree.predict(X_test)\n",
        "limited_acc = accuracy_score(y_test, limited_pred)\n",
        "\n",
        "# 5. Print accuracy comparison\n",
        "print(\"Accuracy of Fully-Grown Tree     :\", full_acc)\n",
        "print(\"Accuracy of max_depth=3 Tree     :\", limited_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EDGnZiLW3Uh",
        "outputId": "3c2c8ae2-eb48-421d-924f-2f28e1ebb6a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Fully-Grown Tree     : 1.0\n",
            "Accuracy of max_depth=3 Tree     : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What this program shows\n",
        "\n",
        "A fully-grown tree may overfit but sometimes gives very high accuracy.\n",
        "\n",
        "A limited-depth tree (max_depth=3):\n",
        "\n",
        "is simpler\n",
        "\n",
        "generalizes better\n",
        "\n",
        "may have slightly lower or sometimes equal accuracy."
      ],
      "metadata": {
        "id": "mTvCv1u7XAGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "‚óè Load the Boston Housing Dataset\n",
        "‚óè Train a Decision Tree Regressor\n",
        "‚óè Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "lgHBeiZzXBjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a clean, updated, and ready-to-run Python program for training a Decision Tree Regressor on the Boston Housing Dataset."
      ],
      "metadata": {
        "id": "mHAENS-uXGh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Boston Housing"
      ],
      "metadata": {
        "id": "ou9mvyYGXe8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load the Boston Housing Dataset\n",
        "boston = load_boston()\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# 2. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Decision Tree Regressor\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict and compute MSE\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "# 5. Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(boston.feature_names, model.feature_importances_):\n",
        "    print(f\"{name}: {importance}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RAvDYc5hXgfn",
        "outputId": "3dd6d53f-2db6-40a5-e728-a5a8bec2654a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-57974539.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_boston\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/datasets/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \"\"\"\n\u001b[1;32m    160\u001b[0m         )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using California Housing Dataset"
      ],
      "metadata": {
        "id": "HsLFHVgUX6_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing Dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and compute MSE\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "# Feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(data.feature_names, model.feature_importances_):\n",
        "    print(f\"{name}: {importance}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx6Z51MDX73w",
        "outputId": "48be0695-68f1-4b4e-d145-47dee915ea63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.495235205629094\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5285090936963706\n",
            "HouseAge: 0.05188353710616045\n",
            "AveRooms: 0.05297496833123543\n",
            "AveBedrms: 0.02866045788296106\n",
            "Population: 0.030515676373806224\n",
            "AveOccup: 0.13083767753210346\n",
            "Latitude: 0.09371656401749287\n",
            "Longitude: 0.08290202505986989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What the Program Does\n",
        "\n",
        "Loads the dataset\n",
        "\n",
        "Splits into training/testing\n",
        "\n",
        "Trains a Decision Tree Regressor\n",
        "\n",
        "Prints:\n",
        "\n",
        "‚úÖ Mean Squared Error\n",
        "\n",
        "‚úÖ Feature importances"
      ],
      "metadata": {
        "id": "h39YCtpBYFov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "‚óè Tune the Decision Tree‚Äôs max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "‚óè Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "SZ7ge-yoYGrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Loads the Iris dataset\n",
        "‚úÖ Tunes max_depth and min_samples_split using GridSearchCV\n",
        "‚úÖ Prints the best parameters and model accuracy\n",
        "\n",
        "‚úÖ Python Program: Decision Tree Hyperparameter Tuning with GridSearchCV"
      ],
      "metadata": {
        "id": "0H2XKs18YLfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define the Decision Tree model\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 4. Define the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# 5. GridSearchCV for tuning\n",
        "grid = GridSearchCV(\n",
        "    estimator=dt,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,               # 5-fold cross-validation\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# 6. Fit the grid search\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# 7. Predict using the best model\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# 8. Compute accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 9. Output results\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQw4dR-7YTzX",
        "outputId": "0ff75fb2-9527-40cf-a219-614b3bd7ea69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What This Program Does\n",
        "\n",
        "Uses GridSearchCV to try multiple combinations of:\n",
        "\n",
        "max_depth\n",
        "\n",
        "min_samples_split\n",
        "\n",
        "Selects the combination with the highest accuracy (CV score)\n",
        "\n",
        "Evaluates that best model on the test set"
      ],
      "metadata": {
        "id": "btl37RAPYZ9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you‚Äôre working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "‚óè Handle the missing values\n",
        "‚óè Encode the categorical features\n",
        "‚óè Train a Decision Tree model\n",
        "‚óè Tune its hyperparameters\n",
        "‚óè Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting."
      ],
      "metadata": {
        "id": "jSG9NcZ0YdGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-by-step process\n",
        "1) Understand the data first\n",
        "\n",
        "Check how many rows/columns, which columns are numeric vs categorical, how much missingness each column has, class balance (disease vs no disease).\n",
        "\n",
        "Ask domain questions: could a missing value be meaningful? (e.g., ‚Äútest not ordered‚Äù may carry info)\n",
        "\n",
        "2) Handle missing values\n",
        "\n",
        "Strategy depends on the column type and why it‚Äôs missing. Common approaches:\n",
        "\n",
        "Numerical features\n",
        "\n",
        "If missingness is small & data missing at random: impute with median (robust) or mean.\n",
        "\n",
        "If distribution is skewed, median is safer.\n",
        "\n",
        "If missingness is informative, add a binary missingness indicator column.\n",
        "\n",
        "For complex patterns, consider KNN imputation or MICE / iterative imputation (can be slower).\n",
        "\n",
        "Categorical features\n",
        "\n",
        "Impute with most frequent (mode) or a new category like \"<missing>\".\n",
        "\n",
        "If missingness itself is meaningful, keep a separate flag.\n",
        "\n",
        "Target (label)\n",
        "\n",
        "Usually drop rows with missing labels (unless you plan weak supervision).\n",
        "\n",
        "Practical rule: Don‚Äôt leak test data: fit imputers on training set only and apply to test set (use Pipelines to enforce this).\n",
        "\n",
        "3) Encode categorical features\n",
        "\n",
        "One-hot encoding: for nominal categorical features with a small number of levels.\n",
        "\n",
        "Ordinal encoding: only if categories have a real order (e.g., stage I/II/III).\n",
        "\n",
        "Target/mean encoding: possible for high-cardinality features but must be used carefully (use cross-validated target encoding to avoid leakage).\n",
        "\n",
        "Use OneHotEncoder(handle_unknown='ignore') to handle unseen categories in test data.\n",
        "\n",
        "4) Train a Decision Tree model\n",
        "\n",
        "Split data: train_test_split(..., stratify=y) to keep class balance.\n",
        "\n",
        "Decision trees don‚Äôt need feature scaling.\n",
        "\n",
        "Consider class imbalance: set class_weight='balanced' or use resampling (SMOTE) if necessary.\n",
        "\n",
        "Start with a simple baseline DecisionTree (e.g., DecisionTreeClassifier(random_state=42)).\n",
        "\n",
        "5) Tune hyperparameters\n",
        "\n",
        "Important hyperparameters to tune:\n",
        "\n",
        "max_depth ‚Äî controls tree depth (prevents overfitting).\n",
        "\n",
        "min_samples_split and min_samples_leaf ‚Äî minimum samples to split / be a leaf.\n",
        "\n",
        "max_features ‚Äî how many features to consider at each split.\n",
        "\n",
        "criterion ‚Äî 'gini' or 'entropy'.\n",
        "\n",
        "ccp_alpha ‚Äî cost-complexity pruning parameter (scikit-learn).\n",
        "\n",
        "Use GridSearchCV or RandomizedSearchCV with cross-validation (e.g., 5-fold stratified CV). Choose scoring metric according to business needs (see next).\n",
        "\n",
        "6) Evaluate performance\n",
        "\n",
        "Use multiple evaluation tools ‚Äî don‚Äôt rely on accuracy alone, especially in healthcare:\n",
        "\n",
        "Classification metrics\n",
        "\n",
        "Recall (sensitivity): fraction of sick patients correctly identified ‚Äî often most important in disease detection (missed cases are costly).\n",
        "\n",
        "Precision: fraction of predicted positives that are true positives.\n",
        "\n",
        "F1 score: balance of precision & recall.\n",
        "\n",
        "ROC AUC: overall ranking ability.\n",
        "\n",
        "PR curve / average precision: better when classes are imbalanced.\n",
        "\n",
        "Other checks\n",
        "\n",
        "Confusion matrix: shows TP, FP, TN, FN.\n",
        "\n",
        "Calibration: are predicted probabilities well calibrated? (important for decision thresholds)\n",
        "\n",
        "Cross-validation: get mean + std of metrics to estimate stability.\n",
        "\n",
        "External validation: test on data from different hospitals / time periods.\n",
        "\n",
        "Explainability: feature importances, SHAP or partial dependence plots ‚Äî essential for clinician trust.\n",
        "\n",
        "Fairness & bias checks: ensure model doesn‚Äôt systematically underperform for protected groups.\n",
        "\n",
        "Monitoring: track model drift and performance in production.\n",
        "\n",
        "7) Deployment & governance considerations (healthcare specifics)\n",
        "\n",
        "Interpretability ‚Äî doctors need explanations (decision paths, feature importances, SHAP).\n",
        "\n",
        "Regulatory & privacy ‚Äî follow HIPAA/GDPR as appropriate.\n",
        "\n",
        "Clinical validation ‚Äî prospective studies, clinician review before deploying.\n",
        "\n",
        "Actionability ‚Äî define clear downstream actions (triage, order confirmatory test, urgent review).\n",
        "\n",
        "8) Business value (real-world)\n",
        "\n",
        "Early detection ‚Üí faster treatment, better outcomes.\n",
        "\n",
        "Triage & prioritization ‚Üí helps prioritize high-risk patients for immediate attention.\n",
        "\n",
        "Cost savings ‚Üí avoid unnecessary tests / reduce late-stage treatment costs.\n",
        "\n",
        "Resource allocation ‚Üí better use of staff and diagnostic equipment.\n",
        "\n",
        "Decision support ‚Üí assist clinicians with second opinions and reduce human error.\n",
        "\n",
        "Population health insights ‚Üí identify risk factors and target preventive interventions.\n",
        "\n",
        "Compact scikit-learn example pipeline\n",
        "\n",
        "This example assumes a binary disease label y and a DataFrame df. It shows separate preprocessing for numeric/categorical, DecisionTree, and GridSearchCV."
      ],
      "metadata": {
        "id": "ELhh2y0UYk_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Example: df is your DataFrame, 'label' is 0/1 disease column\n",
        "# df = pd.read_csv('your_data.csv')\n",
        "# For illustration, assume:\n",
        "# numeric_features = ['age', 'bmi', 'blood_pressure']\n",
        "# categorical_features = ['gender', 'smoking_status']\n",
        "\n",
        "numeric_features = ['age', 'bmi', 'blood_pressure']\n",
        "categorical_features = ['gender', 'smoking_status']\n",
        "\n",
        "X = df[numeric_features + categorical_features]\n",
        "y = df['label']\n",
        "\n",
        "# Train/test split (stratify to keep class balance)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Preprocessing pipelines\n",
        "numeric_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),  # median imputation\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features),\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# Full pipeline with a Decision Tree\n",
        "pipe = Pipeline([\n",
        "    ('pre', preprocessor),\n",
        "    ('clf', DecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'clf__criterion': ['gini', 'entropy'],\n",
        "    'clf__max_depth': [3, 5, 8, None],\n",
        "    'clf__min_samples_split': [2, 5, 10],\n",
        "    'clf__min_samples_leaf': [1, 2, 5],\n",
        "    'clf__class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "grid = GridSearchCV(pipe, param_grid, cv=cv, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_proba = best_model.predict_proba(X_test)[:,1]\n",
        "\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_proba))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBuMSd6XY07N",
        "outputId": "f6966d38-cc12-4d90-d138-4ee8321d769d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
            "Best params: {'clf__class_weight': None, 'clf__criterion': 'gini', 'clf__max_depth': 8, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 5}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5000    0.4545    0.4762        11\n",
            "           1     0.4000    0.4444    0.4211         9\n",
            "\n",
            "    accuracy                         0.4500        20\n",
            "   macro avg     0.4500    0.4495    0.4486        20\n",
            "weighted avg     0.4550    0.4500    0.4514        20\n",
            "\n",
            "ROC AUC: 0.4747474747474747\n",
            "Confusion matrix:\n",
            " [[5 6]\n",
            " [5 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "4bda83ed",
        "outputId": "e08f62b5-0cd8-4cbe-8230-adeb56cd86a2"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a sample DataFrame to simulate the healthcare data\n",
        "data = {\n",
        "    'age': np.random.randint(20, 70, 100),\n",
        "    'bmi': np.random.uniform(18.0, 35.0, 100),\n",
        "    'blood_pressure': np.random.randint(90, 180, 100),\n",
        "    'gender': np.random.choice(['Male', 'Female'], 100),\n",
        "    'smoking_status': np.random.choice(['Never', 'Former', 'Current'], 100),\n",
        "    'label': np.random.randint(0, 2, 100) # 0 for no disease, 1 for disease\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"Sample DataFrame 'df' created:\")\n",
        "display(df.head())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample DataFrame 'df' created:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   age        bmi  blood_pressure  gender smoking_status  label\n",
              "0   69  19.996560             147    Male          Never      1\n",
              "1   42  21.254005             148  Female        Current      1\n",
              "2   34  30.790642             167  Female         Former      0\n",
              "3   32  20.749895             164  Female         Former      1\n",
              "4   23  18.905749             138  Female          Never      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2eb19fd3-4a29-4246-b003-91dc687552cf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>bmi</th>\n",
              "      <th>blood_pressure</th>\n",
              "      <th>gender</th>\n",
              "      <th>smoking_status</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>69</td>\n",
              "      <td>19.996560</td>\n",
              "      <td>147</td>\n",
              "      <td>Male</td>\n",
              "      <td>Never</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>42</td>\n",
              "      <td>21.254005</td>\n",
              "      <td>148</td>\n",
              "      <td>Female</td>\n",
              "      <td>Current</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>34</td>\n",
              "      <td>30.790642</td>\n",
              "      <td>167</td>\n",
              "      <td>Female</td>\n",
              "      <td>Former</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>32</td>\n",
              "      <td>20.749895</td>\n",
              "      <td>164</td>\n",
              "      <td>Female</td>\n",
              "      <td>Former</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>23</td>\n",
              "      <td>18.905749</td>\n",
              "      <td>138</td>\n",
              "      <td>Female</td>\n",
              "      <td>Never</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2eb19fd3-4a29-4246-b003-91dc687552cf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2eb19fd3-4a29-4246-b003-91dc687552cf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2eb19fd3-4a29-4246-b003-91dc687552cf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-20af53be-8ec2-4dc0-b280-84edc8038a8c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-20af53be-8ec2-4dc0-b280-84edc8038a8c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-20af53be-8ec2-4dc0-b280-84edc8038a8c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17,\n        \"min\": 23,\n        \"max\": 69,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          42,\n          23,\n          34\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bmi\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.806426939854558,\n        \"min\": 18.90574857139154,\n        \"max\": 30.79064186321318,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          21.254005223591662,\n          18.90574857139154,\n          30.79064186321318\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"blood_pressure\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12,\n        \"min\": 138,\n        \"max\": 167,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          148,\n          138,\n          167\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gender\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Female\",\n          \"Male\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"smoking_status\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Never\",\n          \"Current\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Short checklist you can follow for a project\n",
        "\n",
        "Data exploration & domain questions\n",
        "\n",
        "Split (train/validation/test) with stratification\n",
        "\n",
        "Build preprocessing pipeline (impute, encode)\n",
        "\n",
        "Baseline model (DecisionTree)\n",
        "\n",
        "Hyperparameter tuning with CV (choose metric carefully)\n",
        "\n",
        "Evaluate with multiple metrics + explainability\n",
        "\n",
        "External validation & fairness checks\n",
        "\n",
        "Deploy with monitoring, clinical validation, and governance"
      ],
      "metadata": {
        "id": "Lr9U7hz5ZK63"
      }
    }
  ]
}